{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4a1d066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# ============ HYPERPARAMS ============\n",
    "DEVICE = \"mps\"  # for macbook training\n",
    "BATCH_SIZE = 64\n",
    "BLOCK_SIZE = 256  # max sequence length for training\n",
    "# =====================================\n",
    "\n",
    "# ============ MODEL HYPERPARAMETERS ============\n",
    "N_EMBD = 384  # embedding dimension\n",
    "N_HEADS = 6  # number of attention heads\n",
    "N_LAYERS = 6  # number of transformer blocks\n",
    "DROPOUT = 0.2  # dropout rate\n",
    "# ================================================\n",
    "\n",
    "# ============ TRAINING HYPERPARAMETERS ============\n",
    "LEARNING_RATE = 3e-4\n",
    "TRAINING_STEPS = 10_000\n",
    "EVAL_ITER_PERIOD = 500\n",
    "EVAL_ITERS = 100  # How many batches to average for stable loss estimate\n",
    "# =================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d496dedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 68\n",
      "Sample encoding: 'Hello' -> [23, 46, 53, 53, 56]\n",
      "Sample decoding: [23, 46, 53, 53, 56] -> 'Hello'\n"
     ]
    }
   ],
   "source": [
    "# ============ TOKENIZER (character-level) ============\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Build vocabulary from characters\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars) + 3  # +3 for special tokens: PAD, BOS, EOS\n",
    "\n",
    "# Special token indices\n",
    "PAD = 0\n",
    "BOS = 1  # Beginning of sequence (used as initial state for insertion)\n",
    "EOS = 2  # End of sequence\n",
    "\n",
    "# Character mappings (offset by 3 for special tokens)\n",
    "char_to_idx = {ch: i + 3 for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i + 3: ch for i, ch in enumerate(chars)}\n",
    "idx_to_char[PAD] = \"<PAD>\"\n",
    "idx_to_char[BOS] = \"<BOS>\"\n",
    "idx_to_char[EOS] = \"<EOS>\"\n",
    "\n",
    "\n",
    "def encode(s: str) -> list[int]:\n",
    "    \"\"\"Encode string to list of token indices\"\"\"\n",
    "    return [char_to_idx[c] for c in s]\n",
    "\n",
    "\n",
    "def decode(tokens: list[int]) -> str:\n",
    "    \"\"\"Decode token indices to string, filtering special tokens\"\"\"\n",
    "    return \"\".join(idx_to_char.get(t, \"?\") for t in tokens if t >= 3)\n",
    "\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Sample encoding: 'Hello' -> {encode('Hello')}\")\n",
    "print(f\"Sample decoding: {encode('Hello')} -> '{decode(encode('Hello'))}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ffe97d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: ABCD\n",
      "Candidate: BD\n",
      "Valid inserts per position:\n",
      "  Position 0: {A}\n",
      "  Position 1: {C}\n",
      "  Position 2: {}\n"
     ]
    }
   ],
   "source": [
    "# ============ INSERTION ORACLE ============\n",
    "# Core algorithm: given a partial sequence (candidate) and target (reference),\n",
    "# compute which tokens can be validly inserted at each position.\n",
    "\n",
    "\n",
    "def get_optimal_inserts(cand: list[int], ref: list[int]) -> list[set[int]]:\n",
    "    \"\"\"\n",
    "    For a candidate sequence that is a subsequence of reference,\n",
    "    compute valid insertions at each position.\n",
    "\n",
    "    Returns: list of sets, one per position in [0, len(cand)+1)\n",
    "             Each set contains tokens that can be inserted at that position\n",
    "             while keeping cand as a subsequence of ref.\n",
    "\n",
    "    Example:\n",
    "        ref  = [A, B, C, D]\n",
    "        cand = [B, D]\n",
    "\n",
    "        Position 0 (before B): can insert A (to get [A,B,D] which is subseq of ref)\n",
    "        Position 1 (between B and D): can insert C\n",
    "        Position 2 (after D): nothing (already at end)\n",
    "\n",
    "        Returns: [{A}, {C}, {}]\n",
    "    \"\"\"\n",
    "    # Find where each cand token appears in ref (leftmost match)\n",
    "    # starts[i] = position in ref after matching cand[:i]\n",
    "    starts = [0]\n",
    "    ref_iter = iter(enumerate(ref))\n",
    "    for cand_item in cand:\n",
    "        for ref_pos, ref_item in ref_iter:\n",
    "            if ref_item == cand_item:\n",
    "                starts.append(ref_pos + 1)\n",
    "                break\n",
    "        else:\n",
    "            raise ValueError(\"cand must be a subsequence of ref\")\n",
    "\n",
    "    # Find rightmost matches going backwards\n",
    "    # ends[i] = position in ref before matching cand[i:]\n",
    "    ends = [len(ref)]\n",
    "    reverse_ref_iter = iter(reversed(list(enumerate(ref))))\n",
    "    for cand_item in reversed(cand):\n",
    "        for ref_pos, ref_item in reverse_ref_iter:\n",
    "            if ref_item == cand_item:\n",
    "                ends.append(ref_pos)\n",
    "                break\n",
    "        else:\n",
    "            raise ValueError(\"cand must be a subsequence of ref\")\n",
    "    ends = ends[::-1]\n",
    "\n",
    "    # Valid inserts at position i are tokens in ref[starts[i]:ends[i]]\n",
    "    inserts = []\n",
    "    for i, j in zip(starts, ends):\n",
    "        inserts.append(set(ref[i:j]))\n",
    "    return inserts\n",
    "\n",
    "\n",
    "# Test the oracle\n",
    "ref = encode(\"ABCD\")\n",
    "cand = encode(\"BD\")\n",
    "print(f\"Reference: {decode(ref)}\")\n",
    "print(f\"Candidate: {decode(cand)}\")\n",
    "inserts = get_optimal_inserts(cand, ref)\n",
    "print(f\"Valid inserts per position:\")\n",
    "for i, s in enumerate(inserts):\n",
    "    print(f\"  Position {i}: {{{', '.join(decode([t]) for t in s)}}}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "67152f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating trajectory for: 'Hello'\n",
      "Reference tokens: [23, 46, 53, 53, 56]\n",
      "\n",
      "Step 0:\n",
      "  Hypothesis: '' (len=0)\n",
      "  Insert 'l' at position 0\n",
      "  Valid tokens at that position: {o, l, e, H}\n",
      "\n",
      "Step 1:\n",
      "  Hypothesis: 'l' (len=1)\n",
      "  Insert 'e' at position 0\n",
      "  Valid tokens at that position: {l, e, H}\n",
      "\n",
      "Step 2:\n",
      "  Hypothesis: 'el' (len=2)\n",
      "  Insert 'o' at position 2\n",
      "  Valid tokens at that position: {o, l}\n",
      "\n",
      "Step 3:\n",
      "  Hypothesis: 'elo' (len=3)\n",
      "  Insert 'H' at position 0\n",
      "  Valid tokens at that position: {H}\n",
      "\n",
      "Step 4:\n",
      "  Hypothesis: 'Helo' (len=4)\n",
      "  Insert 'l' at position 2\n",
      "  Valid tokens at that position: {l}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============ TRAJECTORY GENERATION ============\n",
    "# Generate training samples by simulating insertion trajectories from empty -> reference\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InsertionSample:\n",
    "    \"\"\"A single training sample for the insertion transformer\"\"\"\n",
    "\n",
    "    hypo: list[int]  # Current partial sequence (input)\n",
    "    ref_inserts: list[set[int]]  # Valid insertions at each position (for loss)\n",
    "    chosen_pos: int  # Position where we inserted (target)\n",
    "    chosen_token: int  # Token we inserted (target)\n",
    "\n",
    "\n",
    "def generate_trajectory(ref: list[int], mode: str = \"random\") -> list[InsertionSample]:\n",
    "    \"\"\"\n",
    "    Generate a full trajectory from empty sequence to reference.\n",
    "\n",
    "    Args:\n",
    "        ref: Target sequence (list of token indices)\n",
    "        mode: \"random\" for random order, \"l2r\" for left-to-right\n",
    "\n",
    "    Returns:\n",
    "        List of InsertionSample, one per insertion step\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    hypo = []\n",
    "\n",
    "    while True:\n",
    "        inserts = get_optimal_inserts(hypo, ref)\n",
    "\n",
    "        # Flatten to list of (position, token) pairs\n",
    "        flat_inserts = [\n",
    "            (pos, tok) for pos, tokens in enumerate(inserts) for tok in tokens\n",
    "        ]\n",
    "\n",
    "        if not flat_inserts:\n",
    "            # Trajectory complete - hypo == ref\n",
    "            break\n",
    "\n",
    "        # Choose next insertion\n",
    "        if mode == \"random\":\n",
    "            chosen_pos, chosen_tok = random.choice(flat_inserts)\n",
    "        elif mode == \"l2r\":\n",
    "            # Left-to-right: always insert at position len(hypo), token ref[len(hypo)]\n",
    "            chosen_pos, chosen_tok = len(hypo), ref[len(hypo)]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "        samples.append(\n",
    "            InsertionSample(\n",
    "                hypo=list(hypo),\n",
    "                ref_inserts=inserts,\n",
    "                chosen_pos=chosen_pos,\n",
    "                chosen_token=chosen_tok,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Apply the insertion\n",
    "        hypo.insert(chosen_pos, chosen_tok)\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "# Demonstrate trajectory generation\n",
    "ref = encode(\"Hello\")\n",
    "print(f\"Generating trajectory for: '{decode(ref)}'\")\n",
    "print(f\"Reference tokens: {ref}\\n\")\n",
    "\n",
    "trajectory = generate_trajectory(ref, mode=\"random\")\n",
    "for i, sample in enumerate(trajectory):\n",
    "    print(f\"Step {i}:\")\n",
    "    print(f\"  Hypothesis: '{decode(sample.hypo)}' (len={len(sample.hypo)})\")\n",
    "    print(f\"  Insert '{decode([sample.chosen_token])}' at position {sample.chosen_pos}\")\n",
    "    valid_at_pos = sample.ref_inserts[sample.chosen_pos]\n",
    "    print(\n",
    "        f\"  Valid tokens at that position: {{{', '.join(decode([t]) for t in valid_at_pos)}}}\"\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fb4326c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 1,115,394\n",
      "Train tokens: 1,003,854\n",
      "Val tokens: 111,540\n"
     ]
    }
   ],
   "source": [
    "# ============ TRAINING DATA PREPARATION ============\n",
    "# Convert text into training samples for the insertion transformer\n",
    "\n",
    "# Split data\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "train_size = int(len(data) * 0.9)\n",
    "train_data = data[:train_size]\n",
    "val_data = data[train_size:]\n",
    "\n",
    "print(f\"Total tokens: {len(data):,}\")\n",
    "print(f\"Train tokens: {len(train_data):,}\")\n",
    "print(f\"Val tokens: {len(val_data):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0c2c1206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectory for 'Hi' (with EOS):\n",
      "  Step 0: '' -> insert 'H' at pos 0\n",
      "  Step 1: 'H' -> insert 'i' at pos 1\n",
      "  Step 2: 'Hi' -> insert '<EOS>' at pos 1\n"
     ]
    }
   ],
   "source": [
    "# ============ EOS HANDLING ============\n",
    "# When the hypothesis equals the reference, we need to predict \"FINISH\"\n",
    "# The model outputs logits for (position, token) pairs PLUS a \"finish\" logit\n",
    "\n",
    "\n",
    "def generate_trajectory_with_eos(\n",
    "    ref: list[int], mode: str = \"random\"\n",
    ") -> list[InsertionSample]:\n",
    "    \"\"\"Generate trajectory including the final EOS step.\"\"\"\n",
    "    samples = generate_trajectory(ref, mode=mode)\n",
    "\n",
    "    # Add final step where hypo == ref and we should predict EOS\n",
    "    if ref:\n",
    "        eos_inserts = [{EOS} for _ in range(len(ref) + 1)]\n",
    "        chosen_pos = random.randint(0, len(ref))\n",
    "        samples.append(\n",
    "            InsertionSample(\n",
    "                hypo=list(ref),\n",
    "                ref_inserts=eos_inserts,\n",
    "                chosen_pos=chosen_pos,\n",
    "                chosen_token=EOS,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "# Test\n",
    "ref = encode(\"Hi\")\n",
    "trajectory = generate_trajectory_with_eos(ref)\n",
    "print(f\"Trajectory for 'Hi' (with EOS):\")\n",
    "for i, s in enumerate(trajectory):\n",
    "    tok = \"<EOS>\" if s.chosen_token == EOS else decode([s.chosen_token])\n",
    "    print(f\"  Step {i}: '{decode(s.hypo)}' -> insert '{tok}' at pos {s.chosen_pos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d504e07",
   "metadata": {},
   "source": [
    "# Summary: Training Data Interface\n",
    "\n",
    "We now have the core training data generation for the Insertion Transformer:\n",
    "\n",
    "## Key Components:\n",
    "\n",
    "1. **`get_optimal_inserts(cand, ref)`** - Given a partial sequence (candidate) and target (reference), computes which tokens can be validly inserted at each position.\n",
    "\n",
    "2. **`generate_trajectory(ref)`** - Simulates the full insertion process from empty â†’ reference, returning a list of training samples.\n",
    "\n",
    "3. **`InsertionSample`** - A single training example containing:\n",
    "   - `hypo`: Current partial sequence (model input)\n",
    "   - `ref_inserts`: Valid (position, token) pairs for loss\n",
    "   - `chosen_pos`, `chosen_token`: The action taken (target)\n",
    "\n",
    "4. **`get_batch(split)`** - Returns a batched `InsertionBatch` ready for training.\n",
    "\n",
    "## Model Requirements (Next Steps):\n",
    "\n",
    "The decoder-only insertion transformer needs to:\n",
    "1. Take a partial sequence as input\n",
    "2. Output logits for `(position, token)` pairs: `[batch, num_positions, vocab_size]`\n",
    "3. Plus a \"finish\" logit for EOS prediction\n",
    "\n",
    "The loss will use `valid_mask` to allow credit for ANY valid insertion, not just the one we sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "87dcb380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes:\n",
      "  hypo: torch.Size([4, 15])\n",
      "  hypo_len: tensor([ 2, 11, 12, 15])\n",
      "  target_pos: tensor([ 0,  0, 10,  1])\n",
      "  target_token: tensor([60, 60, 57, 46])\n",
      "  valid_mask: torch.Size([4, 16, 68])\n",
      "\n",
      "Example from batch:\n",
      "  Hypothesis: 'nI'\n",
      "  Insert 's' at position 0\n"
     ]
    }
   ],
   "source": [
    "# ============ BATCH GENERATION ============\n",
    "# For the insertion transformer, each training example is:\n",
    "#   - Input: partial sequence (hypothesis)\n",
    "#   - Target: (position, token) to insert\n",
    "#   - For loss: all valid (position, token) pairs at this step\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InsertionBatch:\n",
    "    \"\"\"A batch of training samples for the insertion transformer\"\"\"\n",
    "\n",
    "    hypo: torch.Tensor  # [B, max_hypo_len] - padded hypotheses\n",
    "    hypo_len: torch.Tensor  # [B] - actual lengths\n",
    "    target_pos: torch.Tensor  # [B] - position to insert\n",
    "    target_token: torch.Tensor  # [B] - token to insert\n",
    "    # For loss computation: valid insertions as sparse tensor\n",
    "    valid_mask: torch.Tensor  # [B, max_hypo_len+1, vocab_size] - 1 where valid\n",
    "\n",
    "\n",
    "def collate_samples(samples: list[InsertionSample]) -> InsertionBatch:\n",
    "    \"\"\"Convert list of samples into a padded batch\"\"\"\n",
    "    B = len(samples)\n",
    "    max_len = max(len(s.hypo) for s in samples) if samples else 0\n",
    "\n",
    "    # Pad hypotheses\n",
    "    hypo = torch.full((B, max_len), PAD, dtype=torch.long)\n",
    "    hypo_len = torch.zeros(B, dtype=torch.long)\n",
    "    target_pos = torch.zeros(B, dtype=torch.long)\n",
    "    target_token = torch.zeros(B, dtype=torch.long)\n",
    "\n",
    "    # Valid mask: [B, max_len+1, vocab_size]\n",
    "    # +1 because we can insert at positions 0 to len(hypo) inclusive\n",
    "    valid_mask = torch.zeros((B, max_len + 1, vocab_size), dtype=torch.bool)\n",
    "\n",
    "    for i, s in enumerate(samples):\n",
    "        L = len(s.hypo)\n",
    "        if L > 0:\n",
    "            hypo[i, :L] = torch.tensor(s.hypo)\n",
    "        hypo_len[i] = L\n",
    "        target_pos[i] = s.chosen_pos\n",
    "        target_token[i] = s.chosen_token\n",
    "\n",
    "        # Fill valid mask\n",
    "        for pos, valid_tokens in enumerate(s.ref_inserts):\n",
    "            for tok in valid_tokens:\n",
    "                valid_mask[i, pos, tok] = True\n",
    "\n",
    "    return InsertionBatch(\n",
    "        hypo=hypo,\n",
    "        hypo_len=hypo_len,\n",
    "        target_pos=target_pos,\n",
    "        target_token=target_token,\n",
    "        valid_mask=valid_mask,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_batch(\n",
    "    split: str,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    block_size: int = BLOCK_SIZE,\n",
    "    include_eos: bool = True,\n",
    ") -> InsertionBatch:\n",
    "    \"\"\"\n",
    "    Get a random batch of training samples.\n",
    "\n",
    "    Strategy:\n",
    "    1. Sample random chunks of text as target sequences\n",
    "    2. For each target, sample ONE random step from its insertion trajectory\n",
    "    3. Include EOS samples so model learns when to stop\n",
    "    \"\"\"\n",
    "    data_source = train_data if split == \"train\" else val_data\n",
    "\n",
    "    samples = []\n",
    "    for _ in range(batch_size):\n",
    "        # Random starting position\n",
    "        start = random.randint(0, len(data_source) - block_size - 1)\n",
    "        ref = [int(t) for t in data_source[start : start + block_size].tolist()]\n",
    "\n",
    "        # Generate trajectory (with EOS) and pick one random sample\n",
    "        if include_eos:\n",
    "            trajectory = generate_trajectory_with_eos(ref, mode=\"random\")\n",
    "        else:\n",
    "            trajectory = generate_trajectory(ref, mode=\"random\")\n",
    "\n",
    "        if trajectory:\n",
    "            sample = random.choice(trajectory)\n",
    "            samples.append(sample)\n",
    "\n",
    "    return collate_samples(samples)\n",
    "\n",
    "\n",
    "# Test batch generation\n",
    "batch = get_batch(\"train\", batch_size=4, block_size=16)\n",
    "print(f\"Batch shapes:\")\n",
    "print(f\"  hypo: {batch.hypo.shape}\")\n",
    "print(f\"  hypo_len: {batch.hypo_len}\")\n",
    "print(f\"  target_pos: {batch.target_pos}\")\n",
    "print(f\"  target_token: {batch.target_token}\")\n",
    "print(f\"  valid_mask: {batch.valid_mask.shape}\")\n",
    "\n",
    "print(f\"\\nExample from batch:\")\n",
    "idx = 0\n",
    "hypo_tokens = [int(t) for t in batch.hypo[idx].tolist()]\n",
    "print(f\"  Hypothesis: '{decode(hypo_tokens)}'\")\n",
    "print(\n",
    "    f\"  Insert '{decode([int(batch.target_token[idx].item())])}' at position {batch.target_pos[idx].item()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e118c550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nI'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(batch.hypo[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0e4eb53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention with BIDIRECTIONAL attention (no causal mask).\n",
    "\n",
    "    Unlike GPT which uses causal masking, the Insertion Transformer uses\n",
    "    bidirectional attention since we need to see all tokens to decide\n",
    "    where to insert.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int, n_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_heads == 0\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = n_embd // n_heads\n",
    "\n",
    "        # Combined QKV projection for efficiency\n",
    "        self.qkv = nn.Linear(n_embd, 3 * n_embd, bias=False)\n",
    "        self.proj = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, mask: torch.Tensor | None = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [B, T, C] input tensor\n",
    "            mask: [B, T] boolean mask, True for valid positions, False for padding\n",
    "        Returns:\n",
    "            [B, T, C] output tensor\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Compute Q, K, V\n",
    "        qkv = self.qkv(x)  # [B, T, 3*C]\n",
    "        qkv = qkv.reshape(B, T, 3, self.n_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # [3, B, n_heads, T, head_dim]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        # Attention scores\n",
    "        scale = 1.0 / math.sqrt(self.head_dim)\n",
    "        attn = (q @ k.transpose(-2, -1)) * scale  # [B, n_heads, T, T]\n",
    "\n",
    "        # Apply padding mask if provided\n",
    "        if mask is not None:\n",
    "            # mask: [B, T] -> [B, 1, 1, T] for broadcasting\n",
    "            attn_mask = mask[:, None, None, :]  # attend TO these positions\n",
    "            attn = attn.masked_fill(~attn_mask, float(\"-inf\"))\n",
    "\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        # Weighted sum\n",
    "        out = attn @ v  # [B, n_heads, T, head_dim]\n",
    "        out = out.transpose(1, 2).reshape(B, T, C)  # [B, T, C]\n",
    "        out = self.proj(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1309b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Simple feed-forward network applied position-wise.\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(n_embd, 4 * n_embd)\n",
    "        self.fc2 = nn.Linear(4 * n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Single transformer block with pre-norm architecture.\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int, n_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.attn = MultiHeadAttention(n_embd, n_heads, dropout)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.mlp = MLP(n_embd, dropout)\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, mask: torch.Tensor | None = None\n",
    "    ) -> torch.Tensor:\n",
    "        x = x + self.attn(self.ln1(x), mask)\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "23cf6643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InsertionTransformer: 10.76M parameters\n",
      "\n",
      "Output shapes:\n",
      "  position_logits: torch.Size([4, 17])\n",
      "  token_logits: torch.Size([4, 17, 68])\n",
      "  insert_logp: torch.Size([4, 17, 68])\n",
      "  finish_logp: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "class InsertionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder-only Insertion Transformer for language modeling.\n",
    "\n",
    "    Key differences from standard GPT:\n",
    "    1. Bidirectional attention (no causal mask) - we see all tokens to decide where to insert\n",
    "    2. Outputs logits for (position, token) pairs\n",
    "    3. Prepends a special \"slot\" token to allow insertion at position 0\n",
    "\n",
    "    Architecture:\n",
    "    - Input: partial sequence [t1, t2, ..., tn]\n",
    "    - Prepend slot: [SLOT, t1, t2, ..., tn] -> n+1 positions\n",
    "    - Each position i outputs:\n",
    "        - position_logit: how likely to insert HERE (before token i)\n",
    "        - token_logits: what token to insert\n",
    "\n",
    "    Output interpretation:\n",
    "    - Position i in output corresponds to inserting BEFORE position i in input\n",
    "    - Position 0 = insert at the beginning\n",
    "    - Position n = insert at the end (after last token)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        n_embd: int = N_EMBD,\n",
    "        n_heads: int = N_HEADS,\n",
    "        n_layers: int = N_LAYERS,\n",
    "        max_seq_len: int = BLOCK_SIZE + 1,\n",
    "        dropout: float = DROPOUT,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_embd = n_embd\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # Token embedding (includes special tokens: PAD=0, BOS=1, EOS=2)\n",
    "        self.tok_emb = nn.Embedding(vocab_size, n_embd)\n",
    "\n",
    "        # Learnable \"slot\" embedding prepended to represent insertion at position 0\n",
    "        self.slot_emb = nn.Parameter(torch.randn(1, 1, n_embd) * 0.02)\n",
    "\n",
    "        # Positional embedding\n",
    "        self.pos_emb = nn.Embedding(max_seq_len, n_embd)\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [TransformerBlock(n_embd, n_heads, dropout) for _ in range(n_layers)]\n",
    "        )\n",
    "\n",
    "        # Final layer norm\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "\n",
    "        # Output heads:\n",
    "        # - token_head: predicts which token to insert [n_embd -> vocab_size]\n",
    "        # - position_head: predicts insertion position weight [n_embd -> 1]\n",
    "        self.token_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "        self.position_head = nn.Linear(n_embd, 1, bias=False)\n",
    "\n",
    "        # Weight tying: share token embeddings with output\n",
    "        self.token_head.weight = self.tok_emb.weight\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hypo: torch.Tensor,  # [B, T] token indices\n",
    "        hypo_len: torch.Tensor | None = None,  # [B] actual lengths\n",
    "    ) -> dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            hypo: [B, T] partial sequence (padded with PAD token)\n",
    "            hypo_len: [B] actual lengths (optional, inferred if not provided)\n",
    "\n",
    "        Returns:\n",
    "            dict with:\n",
    "                - 'position_logits': [B, T+1] logits for each insertion position\n",
    "                - 'token_logits': [B, T+1, vocab_size] logits for each token at each position\n",
    "                - 'insert_logp': [B, T+1, vocab_size] log P(insert token t at position p)\n",
    "                - 'finish_logp': [B] log P(finish/EOS)\n",
    "        \"\"\"\n",
    "        B, T = hypo.shape\n",
    "        device = hypo.device\n",
    "\n",
    "        # Infer lengths from padding if not provided\n",
    "        if hypo_len is None:\n",
    "            hypo_len = (hypo != PAD).sum(dim=1)\n",
    "\n",
    "        # Create attention mask [B, T+1] - True for valid positions (including slot)\n",
    "        # The slot (position 0) is always valid\n",
    "        positions = torch.arange(T, device=device).unsqueeze(0)  # [1, T]\n",
    "        token_mask = positions < hypo_len.unsqueeze(1)  # [B, T]\n",
    "        # Prepend True for slot position\n",
    "        mask = torch.cat(\n",
    "            [torch.ones(B, 1, dtype=torch.bool, device=device), token_mask], dim=1\n",
    "        )  # [B, T+1]\n",
    "\n",
    "        # Token embeddings\n",
    "        tok_emb = self.tok_emb(hypo)  # [B, T, n_embd]\n",
    "\n",
    "        # Prepend slot embedding\n",
    "        slot = self.slot_emb.expand(B, -1, -1)  # [B, 1, n_embd]\n",
    "        x = torch.cat([slot, tok_emb], dim=1)  # [B, T+1, n_embd]\n",
    "\n",
    "        # Add positional embeddings\n",
    "        pos = torch.arange(T + 1, device=device)\n",
    "        x = x + self.pos_emb(pos)\n",
    "\n",
    "        # Apply transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "\n",
    "        x = self.ln_f(x)  # [B, T+1, n_embd]\n",
    "\n",
    "        # Compute logits\n",
    "        position_logits = self.position_head(x).squeeze(-1)  # [B, T+1]\n",
    "        token_logits = self.token_head(x)  # [B, T+1, vocab_size]\n",
    "\n",
    "        # Mask invalid positions (after sequence end + 1 for final insertion point)\n",
    "        # Valid positions: 0 to hypo_len (inclusive), so T+1 positions total for full sequence\n",
    "        pos_mask = torch.arange(T + 1, device=device).unsqueeze(\n",
    "            0\n",
    "        ) <= hypo_len.unsqueeze(1)  # [B, T+1]\n",
    "        position_logits = position_logits.masked_fill(~pos_mask, float(\"-inf\"))\n",
    "\n",
    "        # Compute log probabilities\n",
    "        # P(insert at pos p) comes from position_logits (softmax over positions)\n",
    "        # P(insert token t | pos p) comes from token_logits (softmax over vocab)\n",
    "        position_logp = F.log_softmax(position_logits, dim=-1)  # [B, T+1]\n",
    "        token_logp = F.log_softmax(token_logits, dim=-1)  # [B, T+1, vocab_size]\n",
    "\n",
    "        # Combined: log P(insert token t at position p) = log P(pos) + log P(token|pos)\n",
    "        insert_logp = position_logp.unsqueeze(-1) + token_logp  # [B, T+1, vocab_size]\n",
    "\n",
    "        # Finish probability: inserting EOS at the last valid position\n",
    "        # Actually, in the paper, \"finish\" is predicted when we choose to insert EOS anywhere\n",
    "        # We'll define finish_logp as the logsumexp of inserting EOS at any position\n",
    "        eos_logp = insert_logp[\n",
    "            :, :, EOS\n",
    "        ]  # [B, T+1] - log prob of inserting EOS at each position\n",
    "        finish_logp = torch.logsumexp(\n",
    "            eos_logp.masked_fill(~pos_mask, float(\"-inf\")), dim=1\n",
    "        )  # [B]\n",
    "\n",
    "        return {\n",
    "            \"position_logits\": position_logits,\n",
    "            \"token_logits\": token_logits,\n",
    "            \"insert_logp\": insert_logp,\n",
    "            \"finish_logp\": finish_logp,\n",
    "        }\n",
    "\n",
    "    def get_num_params(self) -> int:\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "\n",
    "# Test the model\n",
    "model = InsertionTransformer(vocab_size=vocab_size)\n",
    "model = model.to(DEVICE)\n",
    "print(f\"InsertionTransformer: {model.get_num_params() / 1e6:.2f}M parameters\")\n",
    "\n",
    "# Test forward pass\n",
    "batch = get_batch(\"train\", batch_size=4, block_size=16)\n",
    "batch_hypo = batch.hypo.to(DEVICE)\n",
    "batch_hypo_len = batch.hypo_len.to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(batch_hypo, batch_hypo_len)\n",
    "\n",
    "print(f\"\\nOutput shapes:\")\n",
    "print(f\"  position_logits: {out['position_logits'].shape}\")\n",
    "print(f\"  token_logits: {out['token_logits'].shape}\")\n",
    "print(f\"  insert_logp: {out['insert_logp'].shape}\")\n",
    "print(f\"  finish_logp: {out['finish_logp'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0e422882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss computation:\n",
      "  Loss: 4.2346\n",
      "  Accuracy: 0.0000\n",
      "  Perplexity: 69.03\n"
     ]
    }
   ],
   "source": [
    "def compute_loss(\n",
    "    model: InsertionTransformer,\n",
    "    batch: InsertionBatch,\n",
    "    device: str = DEVICE,\n",
    ") -> tuple[torch.Tensor, dict]:\n",
    "    \"\"\"\n",
    "    Compute the insertion transformer loss.\n",
    "\n",
    "    Key insight: We give credit for ANY valid insertion, not just the sampled one.\n",
    "    This is done via logsumexp over all valid (position, token) pairs.\n",
    "\n",
    "    Loss = -log P(any valid action)\n",
    "         = -logsumexp_{(p,t) in valid} log P(insert t at p)\n",
    "\n",
    "    For EOS steps (when hypo == ref), the valid action is finishing.\n",
    "    \"\"\"\n",
    "    hypo = batch.hypo.to(device)\n",
    "    hypo_len = batch.hypo_len.to(device)\n",
    "    valid_mask = batch.valid_mask.to(device)  # [B, T+1, vocab_size]\n",
    "    target_token = batch.target_token.to(device)  # [B]\n",
    "\n",
    "    B, T = hypo.shape\n",
    "\n",
    "    # Forward pass\n",
    "    out = model(hypo, hypo_len)\n",
    "    insert_logp = out[\"insert_logp\"]  # [B, T+1, vocab_size]\n",
    "\n",
    "    # Check if this is an EOS step (target is EOS)\n",
    "    is_eos_step = target_token == EOS  # [B]\n",
    "\n",
    "    # For non-EOS steps: loss = -logsumexp over valid insertions\n",
    "    # Mask invalid insertions with -inf before logsumexp\n",
    "    # valid_mask might be smaller than insert_logp if hypo is padded differently\n",
    "    # We need to align them\n",
    "\n",
    "    # Ensure valid_mask matches insert_logp shape\n",
    "    if valid_mask.shape[1] < insert_logp.shape[1]:\n",
    "        # Pad valid_mask\n",
    "        pad_size = insert_logp.shape[1] - valid_mask.shape[1]\n",
    "        valid_mask = F.pad(valid_mask, (0, 0, 0, pad_size), value=False)\n",
    "    elif valid_mask.shape[1] > insert_logp.shape[1]:\n",
    "        # Truncate valid_mask\n",
    "        valid_mask = valid_mask[:, : insert_logp.shape[1], :]\n",
    "\n",
    "    # Mask out invalid insertions\n",
    "    masked_logp = insert_logp.masked_fill(~valid_mask, float(\"-inf\"))\n",
    "\n",
    "    # logsumexp over all valid (position, token) pairs\n",
    "    # Flatten positions and tokens, then logsumexp\n",
    "    logp_any_valid = torch.logsumexp(masked_logp.view(B, -1), dim=-1)  # [B]\n",
    "\n",
    "    # For EOS steps, use finish_logp instead\n",
    "    # Actually, for EOS steps, valid_mask marks EOS as valid at all positions\n",
    "    # So logp_any_valid should already be correct\n",
    "\n",
    "    # Loss is negative log probability\n",
    "    loss = -logp_any_valid.mean()\n",
    "\n",
    "    # Compute some metrics\n",
    "    with torch.no_grad():\n",
    "        # Accuracy: is the argmax a valid action?\n",
    "        pred_flat = insert_logp.view(B, -1).argmax(dim=-1)  # [B]\n",
    "        valid_flat = valid_mask.view(B, -1)  # [B, T*vocab]\n",
    "        acc = valid_flat.gather(1, pred_flat.unsqueeze(1)).float().mean()\n",
    "\n",
    "        # Perplexity-like metric\n",
    "        ppl = torch.exp(loss)\n",
    "\n",
    "    metrics = {\n",
    "        \"loss\": loss.item(),\n",
    "        \"acc\": acc.item(),\n",
    "        \"ppl\": ppl.item(),\n",
    "    }\n",
    "\n",
    "    return loss, metrics\n",
    "\n",
    "\n",
    "# Test loss computation\n",
    "batch = get_batch(\"train\", batch_size=8, block_size=16)\n",
    "loss, metrics = compute_loss(model, batch)\n",
    "print(f\"Test loss computation:\")\n",
    "print(f\"  Loss: {metrics['loss']:.4f}\")\n",
    "print(f\"  Accuracy: {metrics['acc']:.4f}\")\n",
    "print(f\"  Perplexity: {metrics['ppl']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "21e0eaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model: InsertionTransformer) -> dict[str, float]:\n",
    "    \"\"\"Estimate loss over multiple batches for more stable metrics.\"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(EVAL_ITERS)\n",
    "        accs = torch.zeros(EVAL_ITERS)\n",
    "        for k in range(EVAL_ITERS):\n",
    "            batch = get_batch(split, batch_size=BATCH_SIZE, block_size=BLOCK_SIZE)\n",
    "            loss, metrics = compute_loss(model, batch)\n",
    "            losses[k] = metrics[\"loss\"]\n",
    "            accs[k] = metrics[\"acc\"]\n",
    "        out[f\"{split}_loss\"] = losses.mean().item()\n",
    "        out[f\"{split}_acc\"] = accs.mean().item()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a3a5e88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: InsertionTransformer):\n",
    "    \"\"\"Train the insertion transformer.\"\"\"\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    for step in range(TRAINING_STEPS):\n",
    "        # Periodic evaluation\n",
    "        if step % EVAL_ITER_PERIOD == 0 or step == TRAINING_STEPS - 1:\n",
    "            metrics = estimate_loss(model)\n",
    "            print(\n",
    "                f\"Step {step:05d} | \"\n",
    "                f\"Train loss: {metrics['train_loss']:.4f} | \"\n",
    "                f\"Val loss: {metrics['val_loss']:.4f} | \"\n",
    "                f\"Train acc: {metrics['train_acc']:.2%} | \"\n",
    "                f\"Val acc: {metrics['val_acc']:.2%}\"\n",
    "            )\n",
    "            # Generate a sample\n",
    "            sample = generate(model, max_len=50, temperature=0.8)\n",
    "            print(f\"  Sample: '{decode(sample)[:60]}...'\")\n",
    "            print()\n",
    "\n",
    "        # Training step\n",
    "        batch = get_batch(\"train\", batch_size=BATCH_SIZE, block_size=BLOCK_SIZE)\n",
    "        loss, _ = compute_loss(model, batch)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "12778e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation test (untrained model - random output):\n",
      "  Generated 25 tokens: 'yCm,dbB3I-sw;RtNgFXcqthI'\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate(\n",
    "    model: InsertionTransformer,\n",
    "    max_len: int = 50,\n",
    "    temperature: float = 1.0,\n",
    "    device: str = DEVICE,\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Generate a sequence using the insertion transformer.\n",
    "\n",
    "    Starts from empty sequence and iteratively inserts tokens\n",
    "    until EOS is predicted or max_len is reached.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Start with empty sequence\n",
    "    hypo = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        if len(hypo) == 0:\n",
    "            # Empty sequence: create dummy tensor\n",
    "            hypo_tensor = torch.zeros(1, 1, dtype=torch.long, device=device)\n",
    "            hypo_len = torch.tensor([0], device=device)\n",
    "        else:\n",
    "            hypo_tensor = torch.tensor([hypo], dtype=torch.long, device=device)\n",
    "            hypo_len = torch.tensor([len(hypo)], device=device)\n",
    "\n",
    "        # Forward pass\n",
    "        out = model(hypo_tensor, hypo_len)\n",
    "        insert_logp = out[\"insert_logp\"][0]  # [T+1, vocab_size]\n",
    "\n",
    "        # Apply temperature\n",
    "        if temperature != 1.0:\n",
    "            insert_logp = insert_logp / temperature\n",
    "\n",
    "        # Sample from the distribution\n",
    "        # Flatten, sample, then unflatten\n",
    "        num_positions = len(hypo) + 1\n",
    "        logp_flat = insert_logp[:num_positions].reshape(\n",
    "            -1\n",
    "        )  # [num_positions * vocab_size]\n",
    "        probs = F.softmax(logp_flat, dim=0)\n",
    "        idx = torch.multinomial(probs, 1).item()\n",
    "\n",
    "        # Decode position and token\n",
    "        pos = int(idx // model.vocab_size)\n",
    "        tok = int(idx % model.vocab_size)\n",
    "\n",
    "        # Check for EOS\n",
    "        if tok == EOS:\n",
    "            break\n",
    "\n",
    "        # Insert token\n",
    "        hypo.insert(pos, tok)\n",
    "\n",
    "    model.train()\n",
    "    return hypo\n",
    "\n",
    "\n",
    "# Test generation (with untrained model - will be random)\n",
    "print(\"Generation test (untrained model - random output):\")\n",
    "generated = generate(model, max_len=30, temperature=1.0)\n",
    "print(f\"  Generated {len(generated)} tokens: '{decode(generated)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "248a69d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InsertionTransformer: 10.76M parameters\n",
      "Training on 1,003,854 tokens\n",
      "\n",
      "Step 00000 | Train loss: 4.2597 | Val loss: 4.2283 | Train acc: 2.25% | Val acc: 2.83%\n",
      "  Sample: 'qbjMQjbPx...'\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInsertionTransformer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel.get_num_params()\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[32m1e6\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mM parameters\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_data)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tokens\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model)\u001b[39m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28mprint\u001b[39m()\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Training step\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m batch = \u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBLOCK_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m loss, _ = compute_loss(model, batch)\n\u001b[32m     25\u001b[39m optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 81\u001b[39m, in \u001b[36mget_batch\u001b[39m\u001b[34m(split, batch_size, block_size, include_eos)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# Generate trajectory (with EOS) and pick one random sample\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_eos:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     trajectory = \u001b[43mgenerate_trajectory_with_eos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrandom\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     83\u001b[39m     trajectory = generate_trajectory(ref, mode=\u001b[33m\"\u001b[39m\u001b[33mrandom\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mgenerate_trajectory_with_eos\u001b[39m\u001b[34m(ref, mode)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_trajectory_with_eos\u001b[39m(\n\u001b[32m      7\u001b[39m     ref: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m], mode: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mrandom\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[InsertionSample]:\n\u001b[32m      9\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Generate trajectory including the final EOS step.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     samples = \u001b[43mgenerate_trajectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Add final step where hypo == ref and we should predict EOS\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ref:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mgenerate_trajectory\u001b[39m\u001b[34m(ref, mode)\u001b[39m\n\u001b[32m     27\u001b[39m hypo = []\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     inserts = \u001b[43mget_optimal_inserts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhypo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m     \u001b[38;5;66;03m# Flatten to list of (position, token) pairs\u001b[39;00m\n\u001b[32m     33\u001b[39m     flat_inserts = [\n\u001b[32m     34\u001b[39m         (pos, tok) \u001b[38;5;28;01mfor\u001b[39;00m pos, tokens \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(inserts) \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m tokens\n\u001b[32m     35\u001b[39m     ]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mget_optimal_inserts\u001b[39m\u001b[34m(cand, ref)\u001b[39m\n\u001b[32m     51\u001b[39m inserts = []\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(starts, ends):\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     inserts.append(\u001b[38;5;28mset\u001b[39m(ref[i:j]))\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m inserts\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Create a fresh model and train it\n",
    "model = InsertionTransformer(vocab_size=vocab_size)\n",
    "model = model.to(DEVICE)\n",
    "print(f\"InsertionTransformer: {model.get_num_params() / 1e6:.2f}M parameters\")\n",
    "print(f\"Training on {len(train_data):,} tokens\\n\")\n",
    "\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cea6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Generated Samples ===\n",
      "\n",
      "Sample 1:\n",
      " t the  the thee te he be\n",
      "e thetheee te whe t t the th t e wathe\n",
      "----------------------------------------\n",
      "Sample 2:\n",
      " t hhe he th athe  thee the e the hat te t zte\n",
      "hee t see thhheee\n",
      "----------------------------------------\n",
      "Sample 3:\n",
      " he thhe the te the the e se thhoe t the hee dhe hee  the tt the\n",
      "----------------------------------------\n",
      "Sample 4:\n",
      "  t hee the the the be te t Ge wthhen tthe at the t the he  thhe\n",
      "----------------------------------------\n",
      "Sample 5:\n",
      "e tos see be t the she be te the be t thathhe tht e t thee atoe \n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Generate some samples from the trained model\n",
    "print(\"=== Generated Samples ===\\n\")\n",
    "for i in range(5):\n",
    "    sample = generate(model, max_len=100, temperature=0.8)\n",
    "    print(f\"Sample {i + 1}:\")\n",
    "    print(decode(sample))\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8cad13",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
